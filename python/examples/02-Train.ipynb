{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "python_root = '../'\n",
    "sys.path.insert(0, python_root)\n",
    "from model.alexnet import AlexNet\n",
    "from model.triplet_loss import batch_all_triplet_loss, batch_hardest_triplet_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a seed for numpy.\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The virtual-camera images as well as the line data can be read either directly from their location on the disk or from compressed _pickle_ files, that store all these info in a compact way, in a dict-like structure. This representation is particularly useful when the time required to access the file from the disk constitutes the bottleneck of the process (e.g., when training on ETH's cluster Euler and having the data files in a `scratch` folder and the training scripts in the `home` folder): the data is directly loaded in the memory, therefore allowing faster access. To enable this modality, set `read_as_pickle` to `True`. To use regular textfiles and images (generated by the previous steps of the pipeline), set `read_as_pickle` to `False` (make sure to place the data in the correct locations, in particular check that the path `LINESANDIMAGESFOLDER_PATH` in the config file `python/config_paths_and_variables.sh` is set to right value)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Configuration settings (folders, learning parameters, name of the job, etc.)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Configuration settings.\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "from tools import pathconfig\n",
    "\n",
    "# Name of the training job (used to properly name the folders when the results are\n",
    "# stored.)\n",
    "job_name = datetime.now().strftime(\"%d%m%y_%H%M\")\n",
    "\n",
    "# Set read_as_pickle to True to interpret the train/val files below as pickle\n",
    "# files, False to interpret them as regular text files with the format outputted\n",
    "# by split_dataset_with_labels_world.py.\n",
    "read_as_pickle = True\n",
    "\n",
    "if (read_as_pickle):\n",
    "    pickleandsplit_path = pathconfig.obtain_paths_and_variables(\n",
    "            \"PICKLEANDSPLIT_PATH\")\n",
    "    # * Pickle-files version: path of the pickle files to use for training\n",
    "    #       and validation. Note: more than one pickle file at a time can be\n",
    "    #       used for both training and validation. Therefore, train_files\n",
    "    #       and val_files should both be lists.\n",
    "    train_files = [os.path.join(pickleandsplit_path, 'train_0/traj_1/pickled_train.pkl')]\n",
    "    val_files = [os.path.join(pickleandsplit_path, 'train_0/traj_1/pickled_val.pkl')]\n",
    "else:\n",
    "    linesandimagesfolder_path = pathconfig.obtain_paths_and_variables(\n",
    "            \"LINESANDIMAGESFOLDER_PATH\")\n",
    "    # * Textfile version: path to the textfiles for the trainings and\n",
    "    #       validation set.\n",
    "    # TODO: fix to use this non-pickle version. The paths in train_files\n",
    "    # and val_files below should work, but the current version does not\n",
    "    # allow to train on several sets at a time with textfiles. Also,\n",
    "    # textfiles do not contain the endpoints of the lines, but only their\n",
    "    # center point, making it therefore not possible to use the line\n",
    "    # direction/orthonormal representation required by the last version of\n",
    "    # the network.\n",
    "    train_files = [\n",
    "        os.path.join(linesandimagesfolder_path, dataset_name, 'traj_{}'.format(\n",
    "            trajectory), 'train.txt')\n",
    "    ]\n",
    "    val_files = [\n",
    "        os.path.join(linesandimagesfolder_path, dataset_name, 'traj_{}'.format(\n",
    "            trajectory), 'val.txt')\n",
    "    ]\n",
    "    \n",
    "# Either 'bgr' or 'bgr-d': type of the image fed to the network.\n",
    "image_type = 'bgr-d'\n",
    "\n",
    "# Type of line parametrization can be:\n",
    "# * 'direction_and_centerpoint':\n",
    "#      Each line segment is parametrized by its center point and by its unit\n",
    "#      direction vector.  To obtain invariance on the orientation of the\n",
    "#      line (i.e., given the two endpoints we do NOT want to consider one of\n",
    "#      them as the start and the other one as the end of the line segment),\n",
    "#      we enforce that the first entry should be non-negative. => 6\n",
    "#      parameters per line.\n",
    "# * 'orthonormal':\n",
    "#      A line segment is parametrized with a minimum-DOF parametrization\n",
    "#      (4 degrees of freedom) of the infinite line that it belongs to. The\n",
    "#      representation is called orthonormal. => 4 parameters per line.\n",
    "line_parametrization = 'direction_and_centerpoint'\n",
    "\n",
    "# Folder that store the output logs.\n",
    "log_files_folder = \"./logs/\"\n",
    "\n",
    "# Learning parameters.\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 90\n",
    "batch_size = 128\n",
    "# Margin of the triplet loss.\n",
    "margin = 0.2\n",
    "# Either \"batch_all\" or \"batch_hard\". Strategy for triplets selection.\n",
    "triplet_strategy = \"batch_all\"\n",
    "\n",
    "# Network parameters.\n",
    "dropout_rate = 0.5\n",
    "\n",
    "# How often we want to write the tf.summary data to disk.\n",
    "display_step = 1\n",
    "\n",
    "# Path for tf.summary.FileWriter and to store model checkpoints.\n",
    "filewriter_path = os.path.join(log_files_folder, job_name,\n",
    "                               \"triplet_loss_{}\".format(triplet_strategy))\n",
    "checkpoint_path = os.path.join(\n",
    "    log_files_folder, job_name,\n",
    "    \"triplet_loss_{}_ckpt\".format(triplet_strategy))\n",
    "    \n",
    "# Create parent path if it does not exist.\n",
    "if not os.path.isdir(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create network structure__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** List of variables used for training ****\n",
      "conv3/weights:0 --- 884736 parameters\n",
      "conv3/biases:0 --- 384 parameters\n",
      "conv4/weights:0 --- 663552 parameters\n",
      "conv4/biases:0 --- 384 parameters\n",
      "conv5/weights:0 --- 442368 parameters\n",
      "conv5/biases:0 --- 256 parameters\n",
      "fc6/weights:0 --- 37748736 parameters\n",
      "fc6/biases:0 --- 4096 parameters\n",
      "fc7/weights:0 --- 16777216 parameters\n",
      "fc7/biases:0 --- 4096 parameters\n",
      "fc8/weights:0 --- 16834609 parameters\n",
      "fc8/biases:0 --- 4103 parameters\n",
      "fc9/weights:0 --- 262592 parameters\n",
      "fc9/biases:0 --- 64 parameters\n",
      "Total number of parameters is 73627192\n"
     ]
    }
   ],
   "source": [
    "from tools.train_utils import get_train_set_mean\n",
    "\n",
    "# Check if checkpoints already exist.\n",
    "latest_checkpoint = tf.train.latest_checkpoint(checkpoint_path)\n",
    "\n",
    "# Input placeholder.\n",
    "if image_type == 'bgr':\n",
    "    input_img = tf.placeholder(\n",
    "        tf.float32, [None, 227, 227, 3], name=\"input_img\")\n",
    "elif image_type == 'bgr-d':\n",
    "    input_img = tf.placeholder(\n",
    "        tf.float32, [None, 227, 227, 4], name=\"input_img\")\n",
    "\n",
    "# For each line, labels is in the format\n",
    "#   [line_center (3x)] [instance label (1x)]\n",
    "labels = tf.placeholder(tf.float32, [None, 4], name=\"labels\")\n",
    "# Dropout probability.\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "# Line types.\n",
    "line_types = tf.placeholder(tf.float32, [None, 1], name=\"line_types\")\n",
    "# Geometric information.\n",
    "if line_parametrization == 'direction_and_centerpoint':\n",
    "    geometric_info = tf.placeholder(\n",
    "        tf.float32, [None, 6], name=\"geometric_info\")\n",
    "elif line_parametrization == 'orthonormal':\n",
    "    geometric_info = tf.placeholder(\n",
    "        tf.float32, [None, 4], name=\"geometric_info\")\n",
    "else:\n",
    "    raise ValueError(\"Line parametrization should be \"\n",
    "                     \"'direction_and_centerpoint' or 'orthonormal'.\")\n",
    "\n",
    "# Layers for which weights should not be trained.\n",
    "no_train_layers = ['conv1', 'pool1', 'norm1', 'conv2', 'pool2', 'norm2']\n",
    "# Layers for which ImageNet weights should not be loaded.\n",
    "skip_layers = ['fc8', 'fc9']\n",
    "\n",
    "# Initialize model.\n",
    "model = AlexNet(\n",
    "    x=input_img,\n",
    "    line_types=line_types,\n",
    "    geometric_info=geometric_info,\n",
    "    keep_prob=keep_prob,\n",
    "    skip_layer=skip_layers,\n",
    "    input_images=image_type)\n",
    "    \n",
    "# Retrieve embeddings (cluster descriptors) from model output.\n",
    "embeddings = tf.nn.l2_normalize(model.fc9, axis=1)\n",
    "\n",
    "# Get mean of training set if the training is just starting (i.e., if no\n",
    "# previous checkpoints are found).\n",
    "if latest_checkpoint is None:\n",
    "    train_set_mean = get_train_set_mean(\n",
    "        train_files, image_type, read_as_pickle=read_as_pickle)\n",
    "    train_set_mean_tensor = tf.convert_to_tensor(\n",
    "        train_set_mean, dtype=np.float64)\n",
    "    train_set_mean_variable = tf.Variable(\n",
    "        initial_value=train_set_mean_tensor,\n",
    "        trainable=False,\n",
    "        name=\"train_set_mean\")\n",
    "else:\n",
    "    if image_type == 'bgr':\n",
    "        train_set_mean_shape = (3,)\n",
    "    elif image_type == 'bgr-d':\n",
    "        train_set_mean_shape = (4,)\n",
    "    # The value will be restored from the checkpoint.\n",
    "    train_set_mean_variable = tf.get_variable(\n",
    "        name=\"train_set_mean\",\n",
    "        shape=train_set_mean_shape,\n",
    "        dtype=tf.float64,\n",
    "        trainable=False)\n",
    "\n",
    "# List of trainable variables of the layers we want to train.\n",
    "var_list = [\n",
    "    v for v in tf.trainable_variables()\n",
    "    if v.name.split('/')[0] not in no_train_layers\n",
    "]\n",
    "\n",
    "# List of parameters trained.\n",
    "total_parameters = 0\n",
    "print(\"**** List of variables used for training ****\")\n",
    "for var in var_list:\n",
    "    shape = var.get_shape()\n",
    "    var_parameters = 1\n",
    "    for dim in shape:\n",
    "        var_parameters *= dim.value\n",
    "    print(\"{0} --- {1} parameters\".format(var.name, var_parameters))\n",
    "    total_parameters += var_parameters\n",
    "print(\"Total number of parameters is {}\".format(total_parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Define loss and train operation. Also retrieve statistics about the triplets selected while training.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss.\n",
    "with tf.name_scope(\"triplet_loss\"):\n",
    "    if triplet_strategy == \"batch_all\":\n",
    "        (loss, fraction, valid_triplets,\n",
    "         pairwise_dist) = batch_all_triplet_loss(\n",
    "             labels, embeddings, margin=margin, squared=False)\n",
    "    elif triplet_strategy == \"batch_hard\":\n",
    "        (loss, mask_anchor_positive, mask_anchor_negative,\n",
    "         hardest_positive_dist, hardest_negative_dist,\n",
    "         hardest_positive_element, hardest_negative_element,\n",
    "         pairwise_dist) = batch_hardest_triplet_loss(\n",
    "             labels, embeddings, margin=margin, squared=False)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Triplet strategy not recognized: {}\".format(triplet_strategy))\n",
    "    # The following only to assign a name to the tensor.\n",
    "    loss = tf.identity(loss, name=\"train_loss\")\n",
    "# Train operation.\n",
    "with tf.name_scope(\"train\"):\n",
    "    # Get gradients of all trainable variables.\n",
    "    gradients = tf.gradients(loss, var_list)\n",
    "    gradients = list(zip(gradients, var_list))\n",
    "\n",
    "    # Create optimizer and apply gradient descent to the trainable\n",
    "    # variables.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    train_op = optimizer.apply_gradients(grads_and_vars=gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Add variables to summary and initialize the Tensorflow saver__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv3/weights:0/gradient is illegal; using conv3/weights_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv3/biases:0/gradient is illegal; using conv3/biases_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv4/weights:0/gradient is illegal; using conv4/weights_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv4/biases:0/gradient is illegal; using conv4/biases_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv5/weights:0/gradient is illegal; using conv5/weights_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv5/biases:0/gradient is illegal; using conv5/biases_0/gradient instead.\n",
      "INFO:tensorflow:Summary name fc6/weights:0/gradient is illegal; using fc6/weights_0/gradient instead.\n",
      "INFO:tensorflow:Summary name fc6/biases:0/gradient is illegal; using fc6/biases_0/gradient instead.\n",
      "INFO:tensorflow:Summary name fc7/weights:0/gradient is illegal; using fc7/weights_0/gradient instead.\n",
      "INFO:tensorflow:Summary name fc7/biases:0/gradient is illegal; using fc7/biases_0/gradient instead.\n",
      "INFO:tensorflow:Summary name fc8/weights:0/gradient is illegal; using fc8/weights_0/gradient instead.\n",
      "INFO:tensorflow:Summary name fc8/biases:0/gradient is illegal; using fc8/biases_0/gradient instead.\n",
      "INFO:tensorflow:Summary name fc9/weights:0/gradient is illegal; using fc9/weights_0/gradient instead.\n",
      "INFO:tensorflow:Summary name fc9/biases:0/gradient is illegal; using fc9/biases_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv3/weights:0 is illegal; using conv3/weights_0 instead.\n",
      "INFO:tensorflow:Summary name conv3/biases:0 is illegal; using conv3/biases_0 instead.\n",
      "INFO:tensorflow:Summary name conv4/weights:0 is illegal; using conv4/weights_0 instead.\n",
      "INFO:tensorflow:Summary name conv4/biases:0 is illegal; using conv4/biases_0 instead.\n",
      "INFO:tensorflow:Summary name conv5/weights:0 is illegal; using conv5/weights_0 instead.\n",
      "INFO:tensorflow:Summary name conv5/biases:0 is illegal; using conv5/biases_0 instead.\n",
      "INFO:tensorflow:Summary name fc6/weights:0 is illegal; using fc6/weights_0 instead.\n",
      "INFO:tensorflow:Summary name fc6/biases:0 is illegal; using fc6/biases_0 instead.\n",
      "INFO:tensorflow:Summary name fc7/weights:0 is illegal; using fc7/weights_0 instead.\n",
      "INFO:tensorflow:Summary name fc7/biases:0 is illegal; using fc7/biases_0 instead.\n",
      "INFO:tensorflow:Summary name fc8/weights:0 is illegal; using fc8/weights_0 instead.\n",
      "INFO:tensorflow:Summary name fc8/biases:0 is illegal; using fc8/biases_0 instead.\n",
      "INFO:tensorflow:Summary name fc9/weights:0 is illegal; using fc9/weights_0 instead.\n",
      "INFO:tensorflow:Summary name fc9/biases:0 is illegal; using fc9/biases_0 instead.\n"
     ]
    }
   ],
   "source": [
    "# Add gradients to summary.\n",
    "for gradient, var in gradients:\n",
    "    tf.summary.histogram(var.name + '/gradient', gradient)\n",
    "\n",
    "# Add the variables we train to the summary.\n",
    "for var in var_list:\n",
    "    tf.summary.histogram(var.name, var)\n",
    "\n",
    "# Add the loss to summary.\n",
    "if triplet_strategy == \"batch_all\":\n",
    "    tf.summary.scalar('triplet_loss', loss)\n",
    "    tf.summary.scalar('fraction_positive_triplets', fraction)\n",
    "elif triplet_strategy == \"batch_hard\":\n",
    "    tf.summary.scalar('triplet_loss', loss)\n",
    "\n",
    "# Add embedding_mean_norm (should always be 1) to summary.\n",
    "embedding_mean_norm = tf.reduce_mean(tf.norm(embeddings, axis=1))\n",
    "tf.summary.scalar(\"embedding_mean_norm\", embedding_mean_norm)\n",
    "\n",
    "merged_summary = tf.summary.merge_all()\n",
    "\n",
    "# Initialize the FileWriter.\n",
    "writer = tf.summary.FileWriter(filewriter_path)\n",
    "\n",
    "# Initialize an saver to store model checkpoints.\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of train set: [ 33.6551056   34.38172304  32.57309097 634.94343205]\n",
      "Just set data_size to be 9103\n",
      "Just set data_size to be 0\n",
      "2019-01-29 15:50:54.364741 Start training...\n",
      "2019-01-29 15:50:55.247954 Open Tensorboard at --logdir ./logs/290119_1550/triplet_loss_batch_all\n",
      "Starting epoch is 0, num_epochs is 10\n",
      "2019-01-29 15:50:55.248908 Epoch number: 1\n"
     ]
    }
   ],
   "source": [
    "from model.datagenerator import ImageDataGenerator\n",
    "from tools.train_utils import print_batch_triplets_statistics\n",
    "from tools.lines_utils import get_label_with_line_center, get_geometric_info\n",
    "\n",
    "# Set to True to display statistics about the triplets selected while training.\n",
    "display_triplets_statistics = False\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if latest_checkpoint is None:\n",
    "        # No previous checkpoint to start training from => Train from\n",
    "        # scratch.\n",
    "        \n",
    "        # Initialize all variables.\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        # Load the pretrained weights into the layers which are not in\n",
    "        # skip_layers.\n",
    "        model.load_initial_weights(sess)\n",
    "        # Set first epoch to use for training as 0.\n",
    "        starting_epoch = 0\n",
    "    else:\n",
    "        print(\"Found checkpoint {}\".format(latest_checkpoint))\n",
    "        # Load values of variables from checkpoint.\n",
    "        saver.restore(sess, latest_checkpoint)\n",
    "        # Set first epoch to use for training as the number in the last\n",
    "        # checkpoint (note that the number saved in this filename\n",
    "        # corresponds to the number of the last epoch + 1, cf. lines where\n",
    "        # the checkpoints are saved).\n",
    "        start_char = latest_checkpoint.find(\"epoch\")\n",
    "        if start_char == -1:\n",
    "            print(\n",
    "                \"File name of checkpoint is in unexpected format: did not \"\n",
    "                \"find ''epoch''. Exiting.\")\n",
    "            exit()\n",
    "        else:\n",
    "            start_char += 5  # Length of the string 'epoch'\n",
    "            end_char = latest_checkpoint.find(\".ckpt\", start_char)\n",
    "            if end_char == -1:\n",
    "                print(\n",
    "                    \"File name of checkpoint is in unexpected format: did \"\n",
    "                    \"not find ''.ckpt''. Exiting.\")\n",
    "                exit()\n",
    "            else:\n",
    "                starting_epoch = int(latest_checkpoint[start_char:end_char])\n",
    "\n",
    "    # Obtain training set mean.\n",
    "    train_set_mean = sess.run(train_set_mean_variable)\n",
    "    print(\"Mean of train set: {}\".format(train_set_mean))\n",
    "\n",
    "    # Initialize generators for image data.\n",
    "    train_generator = ImageDataGenerator(\n",
    "        files_list=train_files,\n",
    "        horizontal_flip=False,\n",
    "        shuffle=True,\n",
    "        image_type=image_type,\n",
    "        mean=train_set_mean,\n",
    "        read_as_pickle=read_as_pickle)\n",
    "    val_generator = ImageDataGenerator(\n",
    "        files_list=val_files,\n",
    "        shuffle=True,\n",
    "        image_type=image_type,\n",
    "        mean=train_set_mean,\n",
    "        read_as_pickle=read_as_pickle)\n",
    "\n",
    "    # Get the number of training/validation steps per epoch.\n",
    "    train_batches_per_epoch = np.floor(\n",
    "        train_generator.data_size / batch_size).astype(np.int16)\n",
    "    val_batches_per_epoch = np.floor(\n",
    "        val_generator.data_size / batch_size).astype(np.int16)\n",
    "\n",
    "    # Add the model graph to TensorBoard.\n",
    "    writer.add_graph(sess.graph)\n",
    "\n",
    "    print(\"{} Start training...\".format(datetime.now()))\n",
    "    print(\"{} Open Tensorboard at --logdir {}\".format(\n",
    "        datetime.now(), filewriter_path))\n",
    "\n",
    "    print(\"Starting epoch is {0}, num_epochs is {1}\".format(\n",
    "        starting_epoch, num_epochs))\n",
    "\n",
    "    # Loop over number of epochs.\n",
    "    for epoch in range(starting_epoch, num_epochs):\n",
    "        print(\"{} Epoch number: {}\".format(datetime.now(), epoch + 1))\n",
    "        step = 1\n",
    "\n",
    "        while step < train_batches_per_epoch:\n",
    "            # Get a batch of images and labels.\n",
    "            (batch_input_img_train, batch_labels_train,\n",
    "             batch_line_types_train\n",
    "            ) = train_generator.next_batch(batch_size)\n",
    "            # Pickled files have labels in the endpoints format -> convert\n",
    "            # them to center format.\n",
    "            labels_for_stats = batch_labels_train\n",
    "            if read_as_pickle:\n",
    "                batch_start_points_train = batch_labels_train[:, :3]\n",
    "                batch_end_points_train = batch_labels_train[:, 3:6]\n",
    "                batch_geometric_info_train = get_geometric_info(\n",
    "                    start_points=batch_start_points_train,\n",
    "                    end_points=batch_end_points_train,\n",
    "                    line_parametrization=line_parametrization)\n",
    "                batch_labels_train = get_label_with_line_center(\n",
    "                    labels_batch=batch_labels_train)\n",
    "\n",
    "            # Display statistics about triplets.\n",
    "            if (display_triplets_statistics):\n",
    "                if (triplet_strategy == 'batch_all'):\n",
    "                    (pairwise_dist_for_stats,\n",
    "                     valid_triplets_for_stats) = sess.run(\n",
    "                            [pairwise_dist, valid_triplets],\n",
    "                            feed_dict={\n",
    "                                input_img: batch_input_img_train,\n",
    "                                labels: batch_labels_train,\n",
    "                                line_types: batch_line_types_train,\n",
    "                                geometric_info: batch_geometric_info_train,\n",
    "                                keep_prob: dropout_rate\n",
    "                            })\n",
    "                    print_batch_triplets_statistics(\n",
    "                        triplet_strategy=triplet_strategy,\n",
    "                        images=batch_input_img_train,\n",
    "                        set_mean=train_set_mean,\n",
    "                        batch_index=step,\n",
    "                        epoch_index=epoch,\n",
    "                        write_folder='{}_logs/'.format(job_name),\n",
    "                        labels=labels_for_stats,\n",
    "                        pairwise_dist=pairwise_dist_for_stats,\n",
    "                        valid_triplets=valid_triplets_for_stats)\n",
    "                elif (triplet_strategy == 'batch_hard'):\n",
    "                    (pairwise_dist_for_stats,\n",
    "                     mask_anchor_positive_for_stats,\n",
    "                     mask_anchor_negative_for_stats,\n",
    "                     hardest_positive_dist_for_stats,\n",
    "                     hardest_negative_dist_for_stats,\n",
    "                     hardest_positive_element_for_stats,\n",
    "                     hardest_negative_element_for_stats) = sess.run(\n",
    "                         [\n",
    "                             pairwise_dist, mask_anchor_positive,\n",
    "                             mask_anchor_negative, hardest_positive_dist,\n",
    "                             hardest_negative_dist,\n",
    "                             hardest_positive_element,\n",
    "                             hardest_negative_element\n",
    "                         ],\n",
    "                         feed_dict={\n",
    "                             input_img: batch_input_img_train,\n",
    "                             labels: batch_labels_train,\n",
    "                             line_types: batch_line_types_train,\n",
    "                             geometric_info: batch_geometric_info_train,\n",
    "                             keep_prob: dropout_rate\n",
    "                         })\n",
    "                    print_batch_triplets_statistics(\n",
    "                        triplet_strategy=triplet_strategy,\n",
    "                        images=batch_input_img_train,\n",
    "                        set_mean=train_set_mean,\n",
    "                        batch_index=step,\n",
    "                        epoch_index=epoch,\n",
    "                        write_folder='{}_logs/'.format(job_name),\n",
    "                        labels=labels_for_stats,\n",
    "                        pairwise_dist=pairwise_dist_for_stats,\n",
    "                        mask_anchor_positive=mask_anchor_positive_for_stats,\n",
    "                        mask_anchor_negative=mask_anchor_negative_for_stats,\n",
    "                        hardest_positive_dist=\n",
    "                        hardest_positive_dist_for_stats,\n",
    "                        hardest_negative_dist=\n",
    "                        hardest_negative_dist_for_stats,\n",
    "                        hardest_positive_element=\n",
    "                        hardest_positive_element_for_stats,\n",
    "                        hardest_negative_element=\n",
    "                        hardest_negative_element_for_stats)\n",
    "            # Run the training operation.\n",
    "            sess.run(\n",
    "                train_op,\n",
    "                feed_dict={\n",
    "                    input_img: batch_input_img_train,\n",
    "                    labels: batch_labels_train,\n",
    "                    line_types: batch_line_types_train,\n",
    "                    geometric_info: batch_geometric_info_train,\n",
    "                    keep_prob: dropout_rate\n",
    "                })\n",
    "            # Generate summary with the current batch of data and write it\n",
    "            # to file.\n",
    "            if step % display_step == 0:\n",
    "                s = sess.run(\n",
    "                    merged_summary,\n",
    "                    feed_dict={\n",
    "                        input_img: batch_input_img_train,\n",
    "                        labels: batch_labels_train,\n",
    "                        line_types: batch_line_types_train,\n",
    "                        geometric_info: batch_geometric_info_train,\n",
    "                        keep_prob: 1.\n",
    "                    })\n",
    "                writer.add_summary(s,\n",
    "                                    epoch * train_batches_per_epoch + step)\n",
    "\n",
    "            step += 1\n",
    "\n",
    "        # Validate the model on the entire validation set.\n",
    "        print(\"{} Start validation\".format(datetime.now()))\n",
    "        loss_val = 0.\n",
    "        val_count = 0\n",
    "        for _ in range(val_batches_per_epoch):\n",
    "            (batch_input_img_val, batch_labels_val,\n",
    "             batch_line_types_val) = val_generator.next_batch(batch_size)\n",
    "            # Pickled files have labels in the endpoints format -> convert\n",
    "            # them to center format.\n",
    "            if read_as_pickle:\n",
    "                batch_start_points_val = batch_labels_val[:, :3]\n",
    "                batch_end_points_val = batch_labels_val[:, 3:6]\n",
    "                batch_geometric_info_val = get_geometric_info(\n",
    "                    start_points=batch_start_points_val,\n",
    "                    end_points=batch_end_points_val,\n",
    "                    line_parametrization=line_parametrization)\n",
    "                batch_labels_val = get_label_with_line_center(\n",
    "                    labels_batch=batch_labels_val)\n",
    "            # Obtain validation loss.\n",
    "            loss_current = sess.run(\n",
    "                loss,\n",
    "                feed_dict={\n",
    "                    input_img: batch_input_img_val,\n",
    "                    labels: batch_labels_val,\n",
    "                    line_types: batch_line_types_val,\n",
    "                    geometric_info: batch_geometric_info_val,\n",
    "                    keep_prob: 1.\n",
    "                })\n",
    "            loss_val += loss_current\n",
    "            val_count += 1\n",
    "        if val_count != 0:\n",
    "            loss_val = loss_val / val_count\n",
    "            print(\"{} Average loss for validation set = {:.4f}\".format(\n",
    "                datetime.now(), loss_val))\n",
    "            \n",
    "        # Reset the file pointer of the image data generator at the end of\n",
    "        # each epoch.\n",
    "        val_generator.reset_pointer()\n",
    "        train_generator.reset_pointer()\n",
    "\n",
    "        print(\"{} Saving checkpoint of model...\".format(datetime.now()))\n",
    "        # Save checkpoint of the model.\n",
    "        checkpoint_name = os.path.join(\n",
    "            checkpoint_path,\n",
    "            image_type + '_model_epoch' + str(epoch + 1) + '.ckpt')\n",
    "        save_path = saver.save(sess, checkpoint_name)\n",
    "\n",
    "        print(\"{} Model checkpoint saved at {}\".format(\n",
    "              datetime.now(), checkpoint_name))\n",
    "\n",
    "        # The following is useful if one has no access to standard output.\n",
    "        with open(\n",
    "                os.path.join(log_files_folder, job_name,\n",
    "                                \"epochs_completed\"), \"aw\") as f:\n",
    "            f.write(\"Completed epoch {}\\n\".format(epoch + 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "line_tools",
   "language": "python",
   "name": "line_tools"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
